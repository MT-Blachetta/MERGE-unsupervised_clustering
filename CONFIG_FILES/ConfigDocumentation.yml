
#### Global All-dependent
#[model,transformation,loss_function]

train_method: scan

### Mode,Name/ID,Prefix

setup: scan
"""
USE: in congig.py:
scan_dir = os.path.join(base_dir, cfg['setup'])
cfg['scan_dir'] = scan_dir
cfg['scan_checkpoint'] = os.path.join(scan_dir,prefix+'_checkpoint.pth.tar')
cfg['evaluation_dir'] = os.path.join(scan_dir,prefix+'_measurements')
cfg['scan_model'] = os.path.join(scan_dir,prefix+'_model.pth.tar'
"""

### Technical Parameters (unimportant) FIXED
num_workers: 8

### Universal Parameters

epochs: {1,}
batch_size: {4,}
update_cluster_head_only: True 

#### DATASET & TRANSFORM ####

### DATASET related


## !!!fixed!!! dataset related Parameters
dataset_type: scan # Omni-compatible
num_classes: 10
train_db_name: stl-10
val_db_name: stl-10

## nearest neighbors scan loss related
# for model training
neighbor_prefix: scatnet_both
# for validation phase
neighbor_prefix_val: scatnet_both

num_neighbors: 20
to_augmented_dataset: False # mode variable
to_neighbors_dataset: True

## for stl10 dataset
# split of stl10 for training
train_split: both  <-> [neighbor_prefix]
# split of stl10 for validation
val_split: both <-> [neighbor_prefix_val]

### AUGMENTATION related

# <dependent>
augmentation_type: [twist,twist]; [scan] 
augmentation_strategy: [[barlow,moco,random,scan,simclr,standard]; multicrop]; [standard,simclr,ours]
augmentation_kwargs.local_crops_number: [0, {1,}], 0
# <dependent>

# VERSION: default scan loss for stl_10
augmentation_strategy: ours

augmentation_kwargs:
   local_crops_number: 0 
   crop_size: 96
   normalize:
      mean: [0.485, 0.456, 0.406]
      std: [0.229, 0.224, 0.225]
   num_strong_augs: 4
   cutout_kwargs:
     n_holes: 1
     length: 32
     random: True

# VERSION: loss function from twist {barlow,random,moco}
augmentation_kwargs:
    local_crops_number: 0
    color_jitter: 0.4 
    img_size: 96
    global_crops_scale: [0.4, 1.0]
    local_crops_scale: [0.05, 0.4]
    local_crops_size: 32

# !!! FIXED !!! for stl10 validation 
transformation_kwargs: 
   crop_size: 96
   normalize:
      mean: [0.485, 0.456, 0.406]
      std: [0.229, 0.224, 0.225]





### MODEL related

## backbone

# <dependent>
backbone: scatnet
pretrain_path: scatnet.pth # file to load from the backbone directory

feature_dim: 128
scatnet_args: 
    J: 2
    L: 16
    input_size: [96, 96, 3] 
    res_blocks: 30
    out_dim: 128
hidden_dim: 4096 # parameter for [backbone]:twist
# <dependent>


## head

# <dependent>
num_heads: [0 ; {2,}]
model_type: [[mlpHead,twist]; clusterHeads]
# <dependent>

model_args: # softmax is performed over loss function
    head_type: [mlp, linear]
    aug_type: [multicrop, default] <--> (transformation_type)
    batch_norm: False
    last_batchnorm: False
    last_activation: softmax
    drop_out: -1


### Training Algorithm with loss function

train_method: scan
loss_type: scan_loss
entropy_weight: 5.0 # scan_loss parameter
loss_args: # --> loss_type: twist, double
    lam1: 0.0
    lam2: 1.0
    tau: 1.0
    EPS: 0.00001

### Training Process Parameter


## optimizer

# VERSION
optimizer: adam
optimizer_kwargs:
   lr: 0.0001
   weight_decay: 0.0001


# VERSION
optimizer: sgd
optimizer_kwargs:
   nesterov: False
   weight_decay: 0.0001 
   momentum: 0.9
   lr: 0.4


## Scheduler
scheduler: cosine
scheduler_kwargs:
   lr_decay_rate: 0.1



