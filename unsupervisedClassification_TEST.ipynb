{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "99e8db18",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:30: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  method='lar', copy_X=True, eps=np.finfo(np.float).eps,\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:167: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  method='lar', copy_X=True, eps=np.finfo(np.float).eps,\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:284: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  eps=np.finfo(np.float).eps, copy_Gram=True, verbose=0,\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:862: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  eps=np.finfo(np.float).eps, copy_X=True, fit_path=True,\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:1101: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  eps=np.finfo(np.float).eps, copy_X=True, fit_path=True,\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:1127: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  eps=np.finfo(np.float).eps, positive=False):\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:1362: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  max_n_alphas=1000, n_jobs=None, eps=np.finfo(np.float).eps,\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:1602: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  max_n_alphas=1000, n_jobs=None, eps=np.finfo(np.float).eps,\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:1738: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  eps=np.finfo(np.float).eps, copy_X=True, positive=False):\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\decomposition\\online_lda.py:29: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  EPS = np.finfo(np.float).eps\n"
     ]
    }
   ],
   "source": [
    "from cmath import nan\n",
    "import torch\n",
    "import numpy as np\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn import cluster\n",
    "import sklearn\n",
    "from sklearn.decomposition import IncrementalPCA\n",
    "from tqdm import trange, tqdm\n",
    "from scipy.optimize import linear_sum_assignment\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "class Analysator():\n",
    "    def __init__(self,features_,soft_labels,labels_,class_names=['airplane','bird','car','cat','deer','dog','horse','monkey','ship','truck']):\n",
    "          \n",
    "\n",
    "        self.feature_tensor = features_\n",
    "        self.softlabel_tensor = soft_labels\n",
    "        self.confidence_tensor, self.prediction_tensor = torch.max(soft_labels,dim=1)\n",
    "        self.label_tensor = labels_\n",
    "\n",
    "        self.classes = [ class_names[l.item()] for l in self.label_tensor  ]\n",
    "\n",
    "        self.dataset_size = self.label_tensor.shape[0]\n",
    "\n",
    "        self.feature_tensor = torch.nn.functional.normalize(self.feature_tensor, dim = 1)\n",
    "        self.similarity_matrix = torch.einsum('nd,cd->nc', [self.feature_tensor.cpu(), self.feature_tensor.cpu()])\n",
    "\n",
    "        y_train = self.label_tensor.detach().cpu().numpy()\n",
    "        pred = self.prediction_tensor.detach().cpu().numpy()\n",
    "        max_label = max(y_train)\n",
    "        assert(max_label==9)\n",
    "\n",
    "        self.C = get_cost_matrix(pred, y_train, max_label+1)\n",
    "        ri, ci = assign_classes_hungarian(self.C)\n",
    "\n",
    "        self.cluster_to_class = torch.Tensor(ci)\n",
    "        self.correct_samples = self.cluster_to_class[self.prediction_tensor] == self.label_tensor\n",
    "        self.bad_samples = (self.correct_samples == False)\n",
    "        #self.kNN_cosine_similarities = None\n",
    "        self.kNN_indices = None\n",
    "        self.kNN_labels = None\n",
    "        self.kNN_consistent = None\n",
    "        self.kNN_confidences = None\n",
    "        self.proximity = None\n",
    "        self.local_consistency = None\n",
    "        self.criterion_consistent = None\n",
    "        self.knn = 0\n",
    "\n",
    "        #class_names = ['airplane','bird','car','cat','deer','dog','horse','monkey','ship','truck']\n",
    "\n",
    "\n",
    "    #def compute_correct_samples_mask(self):\n",
    "        \n",
    "    #    self.correct_samples = self.cluster_to_class[self.prediction_tensor] == self.label_tensor\n",
    "\n",
    "    def compute_kNN_statistics(self,knn):\n",
    "        self.knn = knn\n",
    "        scores_k, idx_k = self.similarity_matrix.topk(k=knn, dim=1)\n",
    "        self.proximity = torch.mean(scores_k,dim=1)\n",
    "        self.kNN_indices = idx_k\n",
    "        labels_topk = torch.zeros_like(idx_k)\n",
    "        confidence_topk = torch.zeros_like(idx_k,dtype=torch.float)\n",
    "        for s in range(knn):\n",
    "            labels_topk[:, s] = self.prediction_tensor[idx_k[:, s]]\n",
    "            confidence_topk[:, s] = self.confidence_tensor[idx_k[:, s]]\n",
    "        \n",
    "        self.kNN_consistent = labels_topk[:, 0:1] == labels_topk # <boolean mask>\n",
    "        self.local_consistency = self.kNN_consistent.sum(dim=1)/knn\n",
    "        self.kNN_labels = labels_topk\n",
    "        self.kNN_confidences = confidence_topk\n",
    "        # condition = self.kNN_confidences > 0.5\n",
    "        \n",
    "        \n",
    "    def compute_real_consistency(self, criterion):\n",
    "\n",
    "        self.criterion_consistent = []\n",
    "        for i in range(self.dataset_size):\n",
    "            confids = self.kNN_confidences[i][self.kNN_consistent[i]] # +logical_index > +true for index of consistent label; +size=knn > +indexes topk instances\n",
    "            real = confids > criterion\n",
    "            self.criterion_consistent.append(sum(real)/self.knn)\n",
    "\n",
    "        self.criterion_consistent = torch.Tensor(self.criterion_consistent)\n",
    "        \n",
    "        \n",
    "    def get_accuracy(self):\n",
    "\n",
    "        return to_value(sum(self.correct_samples)/len(self.correct_samples))\n",
    "        \n",
    "\n",
    "    def get_meanConfidence_of_consistents(self):\n",
    "        \n",
    "        means = []\n",
    "        for i in range(self.dataset_size):\n",
    "            confids = self.kNN_confidences[i][self.kNN_consistent[i]] # +logical_index > +true for index of consistent label; +size=knn > +indexes topk instances\n",
    "            mean_confidence = sum(confids)/len(confids) # OK\n",
    "            means.append(mean_confidence)\n",
    "\n",
    "        return torch.Tensor(means)\n",
    "        \n",
    "\n",
    "    def get_meanConsistency_of_confidents(self,criterion):\n",
    "\n",
    "        #means = []\n",
    "        conf_mask = self.confidence_tensor > criterion\n",
    "        if sum(conf_mask) == 0: return nan\n",
    "        consistents = self.local_consistency[conf_mask] # generic for selection masks\n",
    "        return to_value(sum(consistents)/len(consistents))\n",
    "        \n",
    "    def num_of_confidents(self,upper,lower=1.0):\n",
    "\n",
    "        upmask = self.confidence_tensor > upper\n",
    "        lowmask = self.confidence_tensor <= lower\n",
    "\n",
    "        conf_mask = upmask*lowmask\n",
    "\n",
    "        return to_value(sum(conf_mask))\n",
    "\n",
    "    def num_of_consistents(self,upper,lower=1.0,real_consistent=False): # divide through dataset_size to get rate/ratio\n",
    "\n",
    "        if real_consistent:\n",
    "\n",
    "            upmask = self.criterion_consistent > upper\n",
    "            lowmask = self.criterion_consistent <= lower\n",
    "\n",
    "            consistent_mask = upmask*lowmask            \n",
    "\n",
    "        else:\n",
    "\n",
    "            upmask = self.local_consistency > upper\n",
    "            lowmask = self.local_consistency <= lower\n",
    "\n",
    "            consistent_mask = upmask*lowmask\n",
    "\n",
    "        return to_value(sum(consistent_mask))        \n",
    "        \n",
    "    def num_reliable_criterion(self,consistent_ratio,confidence_ratio,real_consistent=False):\n",
    "       \n",
    "        confidence_mask = self.select_confident(confidence_ratio)\n",
    "        consistent_mask = self.select_local_consistent(consistent_ratio,real_consistent=real_consistent)\n",
    "\n",
    "        mask = confidence_mask*consistent_mask\n",
    "\n",
    "        return to_value(sum(mask))\n",
    "        \n",
    "\n",
    "    def select_reliable_criterion(self,consistent_ratio,confidence_ratio,real_consistent=False):\n",
    "\n",
    "        confidence_mask = self.select_confident(confidence_ratio)\n",
    "        consistent_mask = self.select_local_consistent(consistent_ratio,real_consistent=real_consistent)\n",
    "\n",
    "        mask = confidence_mask*consistent_mask\n",
    "\n",
    "        return mask.type(torch.bool)        \n",
    "        \n",
    "     \n",
    "\n",
    "    def select_confident(self,upper,lower=1.0):\n",
    "\n",
    "        upmask = self.confidence_tensor > upper\n",
    "        lowmask = self.confidence_tensor <= lower\n",
    "\n",
    "        conf_mask = upmask*lowmask\n",
    "\n",
    "        return conf_mask.type(torch.bool) # in dataset size\n",
    "        \n",
    "\n",
    "    def select_local_consistent(self,upper,lower=1.0,real_consistent=False):\n",
    "\n",
    "        if real_consistent:\n",
    "\n",
    "            upmask = self.criterion_consistent > upper\n",
    "            lowmask = self.criterion_consistent <= lower\n",
    "\n",
    "            consistent_mask = upmask*lowmask\n",
    "\n",
    "        else:\n",
    "\n",
    "            upmask = self.local_consistency > upper\n",
    "            lowmask = self.local_consistency <= lower\n",
    "\n",
    "            consistent_mask = upmask*lowmask\n",
    "\n",
    "        return consistent_mask.type(torch.bool) # in dataset size        \n",
    "                \n",
    "       \n",
    "        \n",
    "    def get_accuracy_from_selection(self,selection_mask):\n",
    "\n",
    "        correct_subset = self.correct_samples[selection_mask]\n",
    "\n",
    "        return to_value(sum(correct_subset)/len(correct_subset))\n",
    "        \n",
    "        \n",
    "    def ratio_from_selection(self,selection_mask,boolean_tensor,relative=True):\n",
    "        \n",
    "        indicators = boolean_tensor[selection_mask]\n",
    "        if len(indicators) == 0: return 0 \n",
    "        \n",
    "        if relative:\n",
    "            return to_value(sum(indicators)/len(indicators))\n",
    "        else:\n",
    "            return to_value(sum(indicators))\n",
    "\n",
    "    def mean_std_from_selection(self,selection_mask,value_tensor):\n",
    "\n",
    "        sub_features = value_tensor[selection_mask]\n",
    "        if len(sub_features) == 0: return nan, nan \n",
    "        std, mean = torch.std_mean(sub_features,unbiased=False)\n",
    "\n",
    "        return to_value(mean), to_value(std)\n",
    "        \n",
    "\n",
    "    def categorical_from_selection(self,selection_mask,category_mapping,values_to_count,mode='ratio+entropy',return_type='list'):\n",
    "\n",
    "        bins = torch.unique(category_mapping)\n",
    "        category_values = category_mapping[selection_mask]\n",
    "\n",
    "        if values_to_count: measurements = values_to_count[selection_mask]\n",
    "        else: measurements = None\n",
    "\n",
    "        total = len(category_values)\n",
    "        if total == 0:\n",
    "            if return_type == 'list': return [str(c.item()) for c in bins], [0 for _ in bins]\n",
    "            if return_type == 'pandas': \n",
    "                if 'entropy' in mode: columnlist = ['entropy', 'mean', 'std', 'min', '25%', '50%', '75%', 'max']\n",
    "                else: columnlist = [ 'mean', 'std', 'min', '25%', '50%', '75%', 'max']\n",
    "                result = pd.Series([0 for _ in bins],index=[str(c.item()) for c in bins])\n",
    "                for n in columnlist: result[n] = nan\n",
    "                return result\n",
    "\n",
    "        category_names = []\n",
    "        amounts = []\n",
    "\n",
    "        for c in bins:\n",
    "\n",
    "            category_names.append(str(c.item()))\n",
    "            counting_mask = category_values == c\n",
    "            end_value = self.count_set(counting_mask,measurements,mode)\n",
    "            amounts.append(end_value)\n",
    "\n",
    "        if return_type == 'list': \n",
    "            return category_names, amounts\n",
    "\n",
    "        elif return_type == 'pandas':\n",
    "            result = pd.Series(amounts,index=category_names)\n",
    "            statistics = result.describe()\n",
    "            statistics['sum'] = result.sum()\n",
    "            #result.update(statistics)\n",
    "            \n",
    "\n",
    "            if 'entropy' in mode:\n",
    "                columnlist = ['entropy', 'mean', 'std', 'min', '25%', '50%', '75%', 'max']\n",
    "                entropy = self.entropy_from_ratios(torch.Tensor(amounts))\n",
    "                statistics['entropy'] = entropy\n",
    "                for name in columnlist:\n",
    "                    result[name] = statistics[name]           \n",
    "\n",
    "            else:                \n",
    "                columnlist = ['mean', 'std', 'min', '25%', '50%', '75%', 'max']\n",
    "                for name in columnlist:\n",
    "                    result[name] = statistics[name]\n",
    "\n",
    "        \n",
    "        return result\n",
    "\n",
    "\n",
    "\n",
    "    def scalar_statistics_from_selection(self,selection_mask,scalar_tensor,parameters):\n",
    "\n",
    "        # init ------------------------------------\n",
    "\n",
    "        if selection_mask is None:\n",
    "            selection_mask = torch.full([self.dataset_size],True)\n",
    "\n",
    "        if parameters['secondary']:\n",
    "            val_range = parameters['range_2']\n",
    "            interval = parameters['interval_2']\n",
    "        else:\n",
    "            val_range = parameters['range']\n",
    "            interval = parameters['interval']\n",
    "\n",
    "\n",
    "        returntype = parameters['return_type']\n",
    "        \n",
    "        if 'count_measure' in parameters.keys(): \n",
    "            values_to_count = parameters['count_measure']\n",
    "            count_mode = parameters['count_mode']\n",
    "            measurements = values_to_count[selection_mask]\n",
    "        else: measurements = None\n",
    "\n",
    "        subset_values = scalar_tensor[selection_mask]\n",
    "        subset_size = len(subset_values)\n",
    "\n",
    "        # initialize index or column names\n",
    "        names = []\n",
    "        values = []\n",
    "        iv = val_range[0]\n",
    "\n",
    "        while iv <= val_range[1]:            \n",
    "            names.append(str(iv))\n",
    "            iv += interval\n",
    "\n",
    "        # zero case ---------------------------------\n",
    "\n",
    "        if subset_size == 0: \n",
    "            #subset_values = torch.Tensor([0])\n",
    "            values = [0 for _ in names]\n",
    "            #numbers = [0 for _ in names]\n",
    "            frequencies = pd.Series(values,index=names)\n",
    "            columnlist = ['count' ,'sum', 'mean', 'std', 'min', '25%', '50%', '75%', 'max']\n",
    "            for n in columnlist: frequencies[n] = nan\n",
    "            return frequencies\n",
    "                \n",
    "        \n",
    "        #return_info = {}\n",
    "        #names = [str(range[0])]\n",
    "        #values = [0]\n",
    "        numbers = []\n",
    "        \n",
    "        #high = max(scalar_tensor)\n",
    "        iv = val_range[0]-interval\n",
    "        \n",
    "\n",
    "        while (iv+interval) <= val_range[1]:\n",
    "            \n",
    "            #names.append(str(iv+interval))\n",
    "            cmask = self.double_condition_mask(iv,iv+interval,subset_values)\n",
    "            counted_value = self.count_set(cmask,measurements,count_mode)\n",
    "            values.append(counted_value)\n",
    "            numbers.append(self.count_double_condition(iv,iv+interval,subset_values))\n",
    "            iv += interval\n",
    "\n",
    "        if returntype == 'dict':\n",
    "            ps = pd.Series(subset_values.detach().cpu().numpy())\n",
    "            description = ps.describe()\n",
    "            resultdict = {'real_values': subset_values, 'intervals':names, 'ratios':values, 'numbers':numbers}\n",
    "            resultdict.update(description)\n",
    "            \n",
    "            return resultdict\n",
    "\n",
    "        #g = nan is set is zero\n",
    "            \n",
    "        elif returntype == 'pandas':\n",
    "            ps = pd.Series(subset_values.detach().cpu().numpy())\n",
    "            description = ps.describe()\n",
    "            frequencies = pd.Series(values,index=names)\n",
    "            columnlist = ['count' ,'sum', 'mean', 'std', 'min', '25%', '50%', '75%', 'max']\n",
    "            #names.extend(columnlist)  # CAN BE USED LATER !\n",
    "            description['sum'] = ps.sum()\n",
    "            for n in columnlist:\n",
    "                frequencies[n] = description[n]\n",
    "\n",
    "            return frequencies\n",
    "\n",
    "        else:\n",
    "            return names, values \n",
    "\n",
    "\n",
    "\n",
    "    def two_scalarSet_statistics_from_selection(self,scalar_features,second_feature,parameters,dataset_mask=None):\n",
    "\n",
    "        result_frame = pd.DataFrame()\n",
    "\n",
    "        if dataset_mask is None:\n",
    "            selected_features = scalar_features\n",
    "            subfeature_values = second_feature\n",
    "            subset_size = len(self.dataset_size)\n",
    "\n",
    "        else:\n",
    "            selected_features = scalar_features[dataset_mask]\n",
    "            subfeature_values = second_feature[dataset_mask]\n",
    "            subset_size = len(subfeature_values)\n",
    "\n",
    "        val_range = parameters['range']\n",
    "        interval = parameters['interval']\n",
    "        #returntype = parameters['return_type'] # pandas or dictionary\n",
    "\n",
    "        \n",
    "        if len(subset_size) == 0: \n",
    "            selected_features = torch.Tensor([0])\n",
    "            subfeature_values = torch.Tensor([0])\n",
    "\n",
    "        #return_info = {}\n",
    "        names = []\n",
    "        #values = [0]\n",
    "        #numbers = [0]\n",
    "\n",
    "        #high = max(scalar_tensor)\n",
    "        iv = val_range[0]\n",
    "        \n",
    "        while iv <= val_range[1]:            \n",
    "            names.append(str(iv))\n",
    "            iv += interval\n",
    "\n",
    "        iv = val_range[0] - interval\n",
    "        parameters['secondary'] = True\n",
    "\n",
    "        #first_set = selected_features == iv\n",
    "        #first_row = self.scalar_statistics_from_selection(first_set,subfeature_values,parameters)\n",
    "        #result_frame = pd.DataFrame(dict(first_row),columns=first_row.index,index=[names[0]])\n",
    "        #ni = 1\n",
    "\n",
    "        while iv+interval <= val_range[1]:\n",
    "\n",
    "            next_set = self.double_condition_mask(iv,iv+interval,selected_features)\n",
    "            next_row = self.scalar_statistics_from_selection(next_set,subfeature_values,parameters)\n",
    "            added = pd.DataFrame(dict(next_row),columns=next_row.index,index=[names[ni]])\n",
    "            result_frame = pd.concat([result_frame,added])\n",
    "            iv += interval\n",
    "    \n",
    "\n",
    "        return result_frame\n",
    "\n",
    "\n",
    "    def two_categorical_statistics_from_selection(self,category_features,second_feature,parameters,dataset_mask=None):\n",
    "\n",
    "        result_frame = pd.DataFrame()\n",
    "\n",
    "        if dataset_mask is None:\n",
    "            selected_features = category_features\n",
    "            subfeature_values = second_feature\n",
    "            subset_size = len(self.dataset_size)\n",
    "\n",
    "        else:\n",
    "            selected_features = category_features[dataset_mask]\n",
    "            subfeature_values = second_feature[dataset_mask]\n",
    "            subset_size = len(subfeature_values)\n",
    "\n",
    "        \n",
    "        #returntype = parameters['return_type'] # pandas or dictionary\n",
    "       \n",
    "        if len(subset_size) == 0: \n",
    "            selected_features = torch.Tensor([0])\n",
    "            subfeature_values = torch.Tensor([0])\n",
    "\n",
    "        bins = torch.unique(category_features)\n",
    "\n",
    "        second_type = parameters['secondary_type']\n",
    "\n",
    "        if second_type == 'categorical':\n",
    "        \n",
    "            vtc = None\n",
    "            mode = parameters['mode'] \n",
    "            rtype = parameters['return_type']\n",
    "            \n",
    "            if 'count_measure' in parameters.keys():\n",
    "                valuesToCount = parameters['count_measure']\n",
    "\n",
    "                # values_to_count is in DATASET-SIZE, so it must be adapted\n",
    "                if dataset_mask is not None: vtc = valuesToCount[dataset_mask]\n",
    "                else: vtc = valuesToCount\n",
    "\n",
    "            for v in bins:\n",
    "                subcategory_mask = selected_features == v\n",
    "                rowSeries = self.categorical_from_selection(subcategory_mask,subfeature_values,values_to_count=vtc,mode=mode,return_type=rtype)\n",
    "                result_frame = pd.concat([result_frame,rowSeries])\n",
    "\n",
    "        else:\n",
    "            #valuesToCount = parameters['']\n",
    "            if 'count_measure' in parameters.keys(): \n",
    "                values_to_count = parameters['count_measure']\n",
    "                if dataset_mask is not None: parameters['count_measure'] = values_to_count[dataset_mask]\n",
    "            \n",
    "            for v in bins:\n",
    "                subcategory_mask = selected_features == v\n",
    "                rowSeries = self.scalar_statistics_from_selection(subcategory_mask,subfeature_values,parameters)\n",
    "                result_frame = pd.concat([result_frame,rowSeries])\n",
    "\n",
    "        return result_frame\n",
    "\n",
    "\n",
    "    def entropy_from_ratios(self,fractions):\n",
    "        return to_value(-sum(fractions*torch.log(fractions)))\n",
    "\n",
    "\n",
    "    def count_double_condition(self,low,high,value_set):\n",
    "\n",
    "        cond1 = value_set <= high \n",
    "        cond2 = value_set > low\n",
    "\n",
    "        return to_value(sum(cond1*cond2))\n",
    "\n",
    "\n",
    "    def double_condition_mask(self,low,high,value_set):\n",
    "\n",
    "        cond1 = value_set <= high \n",
    "        cond2 = value_set > low\n",
    "\n",
    "        mask = cond1*cond2\n",
    "\n",
    "        return mask.type(torch.bool)\n",
    "\n",
    "\n",
    "    def count_set(self,set_mask,measurements,count_mode='mean'):\n",
    "\n",
    "        if sum(set_mask) == 0: return 0\n",
    "\n",
    "        if measurements is None:\n",
    "            if 'ratio' in count_mode:\n",
    "                return to_value(sum(set_mask)/len(set_mask))\n",
    "            else: return to_value(sum(set_mask))\n",
    "\n",
    "        else: \n",
    "\n",
    "            if 'mean' in count_mode:\n",
    "                feature_values = measurements[set_mask]\n",
    "                return to_value(sum(feature_values)/len(feature_values))\n",
    "            else: return to_value(sum(feature_values)) \n",
    "        \n",
    "\n",
    "\n",
    "\n",
    "#------------------------------------------------------------------------------\n",
    "\n",
    "\n",
    "\n",
    "     \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "def topk_consistency(features,predictions,num_neighbors):\n",
    "    \n",
    "    features = torch.nn.functional.normalize(features, dim = 1)\n",
    "    similarity_matrix = torch.einsum('nd,cd->nc', [features.cpu(), features.cpu()])\n",
    "    scores_k, idx_k = similarity_matrix.topk(k=num_neighbors, dim=1)\n",
    "    labels_samples = torch.zeros_like(idx_k)\n",
    "\n",
    "    for s in range(num_neighbors):\n",
    "        labels_samples[:, s] = predictions[idx_k[:, s]]\n",
    "    \n",
    "    true_matrix = labels_samples[:, 0:1] == labels_samples\n",
    "    num_consistent = true_matrix.sum(dim=1)\n",
    "\n",
    "    return num_consistent/num_neighbors\n",
    "\n",
    "\n",
    "\n",
    "def cluster_size_entropy(costmatrix):\n",
    "\n",
    "    absolute = costmatrix.sum(axis=1)\n",
    "    relative = absolute/sum(absolute)\n",
    "    entropy = - sum(relative*np.log(relative))\n",
    "    \n",
    "    return entropy\n",
    "    #class_sum = costmatrix.sum(axis=0)\n",
    "    #sizes = costmatrix.shape\n",
    "\n",
    "    #class_relatives = [ costmatrix[:,i]/class_sum[i] for i in range(sizes[1]) ]\n",
    "    #clas\n",
    "\n",
    "    #for :\n",
    "    #[ costmatrix[:class_id] ]\n",
    "    #costmatrix.sum(axis=0)\n",
    "\n",
    "def confidence_statistic(softmatrix):\n",
    "    max_confidences, _ = torch.max(softmatrix,dim=1)\n",
    "    num_confident_samples = len(torch.where(max_confidences > 0.95)[0])\n",
    "    confidence_ratio = num_confident_samples/len(max_confidences)\n",
    "    confidence_std, confidence_mean = torch.std_mean(max_confidences, unbiased=False)\n",
    "\n",
    "    return confidence_mean, confidence_std, confidence_ratio\n",
    "    \n",
    "\n",
    "\n",
    "def batches(l, n):\n",
    "    for i in range(0, len(l), n): # step_size = n, [i] is a multiple of n\n",
    "        yield l[i:i + n] # teilt das array [l] in batch sub_sequences der Länge n auf\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def get_cost_matrix(y_pred, y, nc=1000): # C[ground-truth_classes,cluster_labels] counts all instances with a given ground-truth and cluster_label\n",
    "    C = np.zeros((nc, y.max() + 1))\n",
    "    for pred, label in zip(y_pred, y):\n",
    "        C[pred, label] += 1\n",
    "    return C \n",
    "\n",
    "\n",
    "\n",
    "def assign_classes_hungarian(C): # rows are (num. of) clusters and columns (num. of) ground-truth classes\n",
    "    row_ind, col_ind = linear_sum_assignment(C, maximize=True) # assume 1200 rows(clusters) and 1000 cols(classes)\n",
    "    ri, ci = np.arange(C.shape[0]), np.zeros(C.shape[0]) # ri contains all CLUSTER/CLASS indexes as integer from 0 / ci the assigned class/cluster --> num_classes\n",
    "    ci[row_ind] = col_ind # assignment of the col_ind[column nr. = CLASS_ID] to the [row nr. = cluster_ID/index]\n",
    "\n",
    "    # ri =: cluster\n",
    "    # ci =: class of cluster corresponded by index\n",
    "\n",
    "    # for overclustering, rest is assigned to best matching class\n",
    "    mask = np.ones(C.shape[0], dtype=bool)\n",
    "    mask[row_ind] = False # True = alle cluster die nicht durch [linear_sum_assignment] einer Klasse zugeordnet wurden\n",
    "    ci[mask] = C[mask, :].argmax(1) # Für weitere Cluster über die Anzahl Klassen hinaus, ordne die Klasse mit der größten Häufigkeit zu \n",
    "    return ri.astype(int), ci.astype(int) # at each position one assignment: ri[x] = index of cluster <--> ci[x] = classID assigned to cluster\n",
    "\n",
    "\n",
    "def assign_classes_majority(C):\n",
    "    col_ind = C.argmax(1) # assign class with the highest occurence to the cluster (row)\n",
    "    row_ind = np.arange(C.shape[0]) # clusterID at position in both arrays (col_ind and row_ind)\n",
    "\n",
    "    # best matching class for every cluster\n",
    "    mask = np.ones(C.shape[0], dtype=bool)\n",
    "    mask[row_ind] = False\n",
    "\n",
    "    return row_ind.astype(int), col_ind.astype(int)\n",
    "\n",
    "\n",
    "\n",
    "#cluster_idx,class_idx = assign_classes_hungarian(C_train)\n",
    "#rid,cid = assign_classes_majority(C_train)\n",
    "\n",
    "def accuracy_from_assignment(C, row_ind, col_ind, set_size=None):\n",
    "    if set_size is None:\n",
    "        set_size = C.sum()\n",
    "    cnt = C[row_ind, col_ind].sum() # sum of all correctly (class)-assigned instances that contributes to the Cluster's ClassID decision\n",
    "    # (that caused the decision)\n",
    "    return cnt / set_size # If all clusters would have only instaces of one unique class, this value becomes = 1\n",
    "\n",
    "def get_best_clusters(C, k=3, formatation=False):\n",
    "    Cpart = C / (C.sum(axis=1, keepdims=True) + 1e-5) # relative Häufigkeit für jedes Cluster label\n",
    "    Cpart[C.sum(axis=1) < 10, :] = 0 # Schwellwert für die Mindestanzahl Instanzen mit ground-truth_class\n",
    "    # setzt bestimmte relative Häufigkeiten auf 0 (aus der Bewertung entfernt)\n",
    "    # print('as', np.argsort(Cpart, axis=None)[::-1])\n",
    "    \n",
    "    # np.argsort(Cpart, axis=None)[::-1] # flattened indices in umgekehrt_absteigender Abfolge (sonst aufsteigender Reihenfolge)\n",
    "    # Cpart.shape = (1000,1000)\n",
    "    ind = np.unravel_index(np.argsort(Cpart, axis=None)[::-1], Cpart.shape)[0]  # first-dimension indices in C of good clusters (highest single frequency correlation)\n",
    "    _, idx = np.unique(ind, return_index=True) # index of the first occurence of the unique element in $[ind]\n",
    "    # idx = 1000 aus einer Million indices (höchst-erst-bestes aus jeder ground-truth), keine Duplikate\n",
    "    cluster_idx = ind[np.sort(idx)]  # unique indices of good clusters (von groß nach klein)\n",
    "    # nimmt den ersten Wert eines auftauchenden classIndex value von [ind] und notiert sich nur die Indexposition in [ind] dabei\n",
    "    # die Werte werden von Beginn bis Ende in der Reihenfolge von [ind] ausgewählt; somit ist der kleinste Wert von idx auch \n",
    "    # der erste Wert von [ind], weitere Werte mit dem gleichen classIndex werden übersprungen und der zweite Werte ist somit der\n",
    "    # nächsthöchste classIndex von [ind], somit hat man die besten classID's in absteigender Reihenfolge    \n",
    "    accs = Cpart.max(axis=1)[cluster_idx] # die accuracies (höchste Wahrscheinlichkeit von Cpart) der besten classes/cluster (als ID)\n",
    "    good_clusters = cluster_idx[:k] # selects the k best clusters\n",
    "    best_acc = Cpart[good_clusters].max(axis=1)\n",
    "    best_class = Cpart[good_clusters].argmax(axis=1)\n",
    "    #print('Best clusters accuracy: {}'.format(best_acc))\n",
    "    #print('Best clusters classes: {}'.format(best_class))\n",
    "    \"\"\"\n",
    "    if formatation:\n",
    "        outstring = ''\n",
    "        for i in range(k):\n",
    "            outstring += str(i)\n",
    "            outstring += ' ,'\n",
    "            outstring += str(good_clusters[i])\n",
    "            outstring += ','\n",
    "            outstring += str(best_class[i])\n",
    "            outstring += ','\n",
    "            outstring += str(best_acc[i])        \n",
    "            outstring += '\\n'\n",
    "            \n",
    "        print(outstring)\n",
    "    \"\"\"\n",
    "  \n",
    "    return {'best_clusters': good_clusters, 'classes': best_class, 'accuracies': best_acc}\n",
    "\n",
    "\n",
    "def train_pca(X_train,n_comp):\n",
    "    bs = max(4096, X_train.shape[1] * 2)\n",
    "    transformer = IncrementalPCA(batch_size=bs,n_components=n_comp)  #\n",
    "    for i, batch in enumerate(tqdm(batches(X_train, bs), total=len(X_train) // bs + 1)):\n",
    "        transformer = transformer.partial_fit(batch)\n",
    "        # break\n",
    "    print(transformer.explained_variance_ratio_.cumsum())\n",
    "    return transformer\n",
    "\n",
    "def transform_pca(X, transformer):\n",
    "    n = max(4096, X.shape[1] * 2)\n",
    "    n_comp = transformer.components_.shape[0]\n",
    "    X_ = np.zeros((X.shape[0],n_comp))\n",
    "    for i in trange(0, len(X), n):\n",
    "        X_[i:i + n] = transformer.transform(X[i:i + n])\n",
    "        # break\n",
    "    return X_\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def evaluate_singleHead(device,model,dataloader,forwarding='head',formatation=False):\n",
    "\n",
    "    model.eval()\n",
    "    predictions = []\n",
    "    labels = []\n",
    "    features = []\n",
    "    soft_labels = []\n",
    "\n",
    "    model.to(device)\n",
    "    #type_test = next(iter(dataloader))\n",
    "    #isinstance\n",
    "\n",
    "    with torch.no_grad():      \n",
    "        for batch in dataloader:\n",
    "            if isinstance(batch,dict):\n",
    "                image = batch['image']\n",
    "                label = batch['target']\n",
    "            else:\n",
    "                image = batch[0]\n",
    "                label = batch[1]\n",
    "\n",
    "            image = image.to(device,non_blocking=True)\n",
    "            fea = model(image,forward_pass='features')\n",
    "            features.append(fea)\n",
    "            predic = model(fea,forward_pass=forwarding)\n",
    "            soft_labels.append(predic)\n",
    "            predictions.append(torch.argmax(predic, dim=1))\n",
    "            labels.append(label)\n",
    "\n",
    "    feature_tensor = torch.cat(features)\n",
    "    softlabel_tensor = torch.cat(soft_labels)\n",
    "    prediction_tensor = torch.cat(predictions)\n",
    "    label_tensor = torch.cat(labels)\n",
    "\n",
    "    consistency_values = topk_consistency(feature_tensor,prediction_tensor,100)\n",
    "    consistency_ratio = len(torch.where(consistency_values > 0.5)[0])/len(consistency_values)\n",
    "    print('consistency_ratio = ',consistency_ratio)\n",
    "    #c_std, c_mean =  torch.std_mean(consistency_values, unbiased=False)\n",
    "    \n",
    "    y_train = label_tensor.detach().cpu().numpy()\n",
    "    pred = prediction_tensor.detach().cpu().numpy()\n",
    "    max_label = max(y_train)\n",
    "    assert(max_label==9)\n",
    "\n",
    "    C_train = get_cost_matrix(pred, y_train, max_label+1)\n",
    "\n",
    "    cluster_entropy = cluster_size_entropy(C_train)\n",
    "    conf_mean, conf_std, conf_rate = confidence_statistic(softlabel_tensor)\n",
    "    print('confidence_rate = ',conf_rate)\n",
    "\n",
    "    result_dict = {'cluster_size_entropy': cluster_entropy, 'confidence_ratio': conf_rate , 'mean_confidence': conf_mean.item(), 'std_confidence': conf_std.item(), 'consistency_ratio': consistency_ratio}\n",
    "\n",
    "    message = 'val'\n",
    "    y_pred = pred\n",
    "    y_true = y_train\n",
    "    train_lin_assignment = assign_classes_hungarian(C_train)\n",
    "    #train_maj_assignment = assign_classes_majority(C_train)\n",
    "\n",
    "    acc_tr_lin = accuracy_from_assignment(C_train, *train_lin_assignment)\n",
    "    #acc_tr_maj = accuracy_from_assignment(C_train, *train_maj_assignment)\n",
    "\n",
    "    #result_dict = get_best_clusters(C_train,k=10,formatation=formatation)\n",
    "\n",
    "\n",
    "    ari = sklearn.metrics.adjusted_rand_score(y_true, y_pred)\n",
    "    v_measure = sklearn.metrics.v_measure_score(y_true, y_pred)\n",
    "    ami = sklearn.metrics.adjusted_mutual_info_score(y_true, y_pred)\n",
    "    fm = sklearn.metrics.fowlkes_mallows_score(y_true, y_pred)\n",
    "\n",
    "    #headline = 'method,ACC,ARI,AMI,FowlkesMallow,'\n",
    "    #print('\\ncluster performance:\\n')\n",
    "    #print(eval_name+'  ,'+str(acc_tr_lin)+', '+str(ari)+', '+str(v_measure)+', '+str(ami)+', '+str(fm))\n",
    "\n",
    "    result_dict['Accuracy'] = acc_tr_lin\n",
    "    result_dict['Adjusted_Random_Index'] = ari\n",
    "    result_dict['V_measure'] = v_measure\n",
    "    result_dict['fowlkes_mallows'] = fm\n",
    "    result_dict['Adjusted_Mutual_Information'] = ami\n",
    "\n",
    "    print(\"\\n{}: ARI {:.5e}\\tV {:.5e}\\tAMI {:.5e}\\tFM {:.5e}\\tACC {:.5e}\".format(message, ari, v_measure, ami, fm, acc_tr_lin))\n",
    "\n",
    "    return result_dict\n",
    "\n",
    "\n",
    "def evaluate_prediction(y_true,y_pred,formatation=False):\n",
    "\n",
    "    max_label = max(y_true)\n",
    "    assert(max_label==9)\n",
    "    #print(y_true)\n",
    "    C_train = get_cost_matrix(y_pred, y_true, max_label+1)\n",
    "\n",
    "    #message = 'val'\n",
    "    #y_pred = pred\n",
    "    #y_true = y_train\n",
    "    train_lin_assignment = assign_classes_hungarian(C_train)\n",
    "    train_maj_assignment = assign_classes_majority(C_train)\n",
    "\n",
    "    acc_tr_lin = accuracy_from_assignment(C_train, *train_lin_assignment)\n",
    "    #acc_tr_maj = accuracy_from_assignment(C_train, *train_maj_assignment)\n",
    "\n",
    "    result_dict = get_best_clusters(C_train,k=10,formatation=formatation)\n",
    "\n",
    "\n",
    "    ari = sklearn.metrics.adjusted_rand_score(y_true, y_pred)\n",
    "    v_measure = sklearn.metrics.v_measure_score(y_true, y_pred)\n",
    "    ami = sklearn.metrics.adjusted_mutual_info_score(y_true, y_pred)\n",
    "    fm = sklearn.metrics.fowlkes_mallows_score(y_true, y_pred)\n",
    "\n",
    "    #headline = 'method,ACC,ARI,AMI,FowlkesMallow,'\n",
    "    #print('\\ncluster performance:\\n')\n",
    "    #print(eval_name+'  ,'+str(acc_tr_lin)+', '+str(ari)+', '+str(v_measure)+', '+str(ami)+', '+str(fm))\n",
    "\n",
    "    result_dict['ACC'] = acc_tr_lin\n",
    "    result_dict['ARI'] = ari\n",
    "    result_dict['V_measure'] = v_measure\n",
    "    result_dict['fowlkes_mallows'] = fm\n",
    "    result_dict['AMI'] = ami\n",
    "\n",
    "    #print(\"\\n{}: ARI {:.5e}\\tV {:.5e}\\tAMI {:.5e}\\tFM {:.5e}\\tACC {:.5e}\".format(message, ari, v_measure, ami, fm, acc_tr_lin))\n",
    "\n",
    "    return result_dict\n",
    "\n",
    "\n",
    "def evaluate_headlist(device,model,dataloader,formatation=False):\n",
    "\n",
    "    predictions = [ [] for _ in range(model.nheads) ]\n",
    "    label_list = []\n",
    "    #labels = [ [] for _ in range(model.nheads)]\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "\n",
    "    with torch.no_grad(): \n",
    "        for batch in dataloader:\n",
    "            if isinstance(batch,dict):\n",
    "                image = batch['image']\n",
    "                labels = batch['target']\n",
    "            else:\n",
    "                image = batch[0]\n",
    "                labels = batch[1]\n",
    "\n",
    "            label_list.append(labels)\n",
    "            image = image.to(device,non_blocking=True)\n",
    "            predlist = model(image,forward_pass='eval')\n",
    "            for k in range(len(predlist)):\n",
    "                predictions[k].append(predlist[k])\n",
    "\n",
    "    targets = torch.cat(label_list)\n",
    "    targets = targets.detach().cpu().numpy()\n",
    "    #print('targets.shape: ', targets.shape)\n",
    "    headlist = [torch.cat(pred) for pred in predictions]\n",
    "    head_labels = [torch.argmax(softlabel,dim=1) for softlabel in headlist]\n",
    "    #print('len = ',len(headlist))\n",
    "    #print('predictions.shape: ',headlist[0].shape)\n",
    "\n",
    "    accuracies = []\n",
    "    dicts = []\n",
    "    for h in head_labels:\n",
    "        rdict = evaluate_prediction(targets,h.detach().cpu().numpy())\n",
    "        accuracies.append(rdict['ACC'])\n",
    "        #print(rdict['ACC'])\n",
    "        dicts.append(rdict)\n",
    "\n",
    "    best_head = np.argmax(np.array(accuracies))\n",
    "    best_accuracy = max(accuracies)\n",
    "\n",
    "    #result_dict = dicts[best_head]\n",
    "    #result_dict['head_id'] = best_head\n",
    "\n",
    "    #acc_tr_lin = result_dict['ACC'] \n",
    "    #ari = result_dict['ARI'] \n",
    "    #v_measure = result_dict['V_measure']\n",
    "    #fm = result_dict['fowlkes_mallows']\n",
    "    #ami = result_dict['AMI']\n",
    "\n",
    "    #message = 'validation'\n",
    "\n",
    "    print('best accuracy: ', best_accuracy,'  on head ',best_head)\n",
    "    #print('best head is ',best_head)\n",
    "    #print(\"\\n{}: ARI {:.5e}\\tV {:.5e}\\tAMI {:.5e}\\tFM {:.5e}\\tACC {:.5e}\".format(message, ari, v_measure, ami, fm, acc_tr_lin))\n",
    "\n",
    "    return dicts\n",
    "\n",
    "\n",
    "#def negate(boolean):\n",
    "#    return not boolean\n",
    "\n",
    "def to_value(v):\n",
    "    if isinstance(v,torch.Tensor):\n",
    "        v = v.item()        \n",
    "    return v\n",
    "        \n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f620bc03",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "features = torch.rand([5000,128])\n",
    "features = torch.nn.functional.normalize(features,dim=1)\n",
    "softlabels = torch.rand([5000,10])\n",
    "softlabels = torch.softmax(softlabels,dim=1)\n",
    "labels = torch.randint(0,10,[5000])\n",
    "stats = Analysator(features,softlabels,labels)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d9c897e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "stats.compute_kNN_statistics(100)\n",
    "stats.compute_real_consistency(0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7c2a7643",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "nan"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stats.get_meanConsistency_of_confidents(0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0c21b662",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([False, False, False,  ..., False, False, False])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out = stats.confidence_tensor > 0.1\n",
    "out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9bc368ea",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "too many dimensions 'str'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_80612\\4111177767.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m#stats.get_accuracy_from_selection(stats.bad_samples)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'first'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'second'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'third'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m: too many dimensions 'str'"
     ]
    }
   ],
   "source": [
    "#stats.get_accuracy_from_selection(stats.bad_samples)\n",
    "torch.Tensor(['first','second','third'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a0558a55",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Boolean value of Tensor with more than one value is ambiguous",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_80612\\2135783307.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m#stats.mean_std_from_selection(stats.bad_samples,stats.confidence_tensor)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mstats\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcategorical_from_selection\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstats\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbad_samples\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mstats\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlabel_tensor\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mstats\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconfidence_tensor\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mmode\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'mean'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mreturn_type\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'pandas'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_80612\\384782629.py\u001b[0m in \u001b[0;36mcategorical_from_selection\u001b[1;34m(self, selection_mask, category_mapping, values_to_count, mode, return_type)\u001b[0m\n\u001b[0;32m    217\u001b[0m         \u001b[0mcategory_values\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcategory_mapping\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mselection_mask\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    218\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 219\u001b[1;33m         \u001b[1;32mif\u001b[0m \u001b[0mvalues_to_count\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mmeasurements\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mvalues_to_count\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mselection_mask\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    220\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mmeasurements\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    221\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: Boolean value of Tensor with more than one value is ambiguous"
     ]
    }
   ],
   "source": [
    "#stats.mean_std_from_selection(stats.bad_samples,stats.confidence_tensor)\n",
    "stats.categorical_from_selection(stats.bad_samples,stats.label_tensor,stats.confidence_tensor,mode='mean',return_type='pandas')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e2ffd90e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([False, False, False,  ..., False, False, False])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "stats.compute_correct_samples_mask()\n",
    "stats.correct_samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7a00f05a",
   "metadata": {},
   "outputs": [],
   "source": [
    "stats.compute_kNN_statistics(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "13bb4d29",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.1430, 0.1463, 0.1439,  ..., 0.1370, 0.1465, 0.1471])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "props = stats.get_meanConfidence_of_consistents()\n",
    "props"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f0c83356",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['0', '1', '2', '3', '4', '5', '6', '7', '8', '9']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#mcocs = stats.get_meanConfidence_of_consistents()\n",
    "#mcocs\n",
    "inspect = stats.select_confident(0.15)\n",
    "#cc = stats.label_tensor\n",
    "a,b = stats.categorical_from_selection(inspect,stats.label_tensor)\n",
    "a\n",
    "#type(inspect)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "90009ed1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.09448818862438202,\n",
       " 0.0938824936747551,\n",
       " 0.09872804582118988,\n",
       " 0.10417928546667099,\n",
       " 0.10841914266347885,\n",
       " 0.10054512321949005,\n",
       " 0.10781344771385193,\n",
       " 0.10236220806837082,\n",
       " 0.09448818862438202,\n",
       " 0.09509388357400894]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "9496ea31",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([5, 1, 6,  ..., 8, 9, 1])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#bins = torch.unique(stats.label_tensor)\n",
    "#bins\n",
    "\n",
    "#for v in bins: print(v)\n",
    "category_values = stats.label_tensor[inspect]\n",
    "category_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "7d446813",
   "metadata": {},
   "outputs": [],
   "source": [
    "confids = stats.kNN_confidences[0][stats.kNN_consistent[0]]\n",
    "#for i in range(stats.dataset_size): print(sum(stats.kNN_consistent[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "f0d102cb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.1319, 0.1391, 0.1672, 0.1341, 0.1525, 0.1261, 0.1431, 0.1498, 0.1625,\n",
       "        0.1493])"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "confids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "a27e1c0c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.1455)"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mean_confidence = sum(confids)/len(confids)\n",
    "mean_confidence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5bb3c580",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "only integer tensors of a single element can be converted to an index",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_80612\\3662589042.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mminitest\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m3\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m4\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m5\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m6\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mminitest\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m: only integer tensors of a single element can be converted to an index"
     ]
    }
   ],
   "source": [
    "minitest = [1,2,3,4,5,6]\n",
    "minitest[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "80c94124",
   "metadata": {},
   "outputs": [],
   "source": [
    "testtensor = torch.Tensor([False,False,True,True,True,False])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "be1b06a8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 0, 1, 1, 1, 0]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "testlist = [int(t.item()) for t in testtensor]\n",
    "testlist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d25c9f95",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[3, 4, 5]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "helplist =\n",
    "[minitest[i] for i in range(len(testlist)) if testlist[i] > 0.5 ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83d08f16",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
