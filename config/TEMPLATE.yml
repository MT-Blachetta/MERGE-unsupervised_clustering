setup: scan_scatnet

### Technical Parameters (unimportant) FIXED
num_workers: 8

### Universal Parameters
epochs: 2
batch_size: 256
update_cluster_head_only: False

num_classes: 10
train_db_name: stl-10
val_db_name: stl-10

## nearest neighbors scan loss related
# for model training
neighbor_prefix: scatnet_both
# for validation phase
neighbor_prefix_val: scatnet_both
num_neighbors: 20

to_augmented_dataset: False # mode variable
to_neighbors_dataset: True
## for stl10 dataset
# split of stl10 for training
train_split: both
# split of stl10 for validation
val_split: both



# Augmentation
augmentation_type: scan 
augmentation_strategy: ours
aug_type: default
augmentation_kwargs:
   crop_size: 96 # SCAN 
   normalize: # SCAN
      mean: [0.485, 0.456, 0.406]
      std: [0.229, 0.224, 0.225]
   num_strong_augs: 4 # SCAN
   cutout_kwargs: # SCAN
     n_holes: 1
     length: 32
     random: True   
    color_jitter: 0.4 # TWIST
    img_size: 96 # TWIST
    global_crops_scale: [0.4, 1.0] # TWIST
    local_crops_scale: [0.05, 0.4] # TWIST
    local_crops_number: 4 # TWIST
    local_crops_size: 32 # TWIST
    
    
### cluster_modell
## BACKBONE
backbone: scatnet
pretrain_path: scatnet.pth # file to load from the backbone directory
feature_dim: 128
scatnet_args: 
    J: 2
    L: 16
    input_size: [96, 96, 3] 
    res_blocks: 30
    out_dim: 128
## HEAD
num_heads: 10
model_type: clusterHeads
model_args: # softmax is performed over loss function
    head_type: mlp
    batch_norm: False
    last_batchnorm: False
    last_activation: softmax
    drop_out: -1
    
hidden_dim: 4096
###
    
    
    
### Loss & Training Method
loss_args:
    lam1: 0.0
    lam2: 1.0
    tau: 1.0
    EPS: 0.00001
train_method: scan
loss_type: scan_loss
entropy_weight: 5.0 

## optimizer
optimizer: adam
optimizer_kwargs:
   lr: 0.0001
   weight_decay: 0.0001
## Scheduler
scheduler: constant
scheduler_kwargs:
   lr_decay_rate: 0.1
    


#hidden_dim: 4096 # parameter for [backbone]:twist
# <dependent>