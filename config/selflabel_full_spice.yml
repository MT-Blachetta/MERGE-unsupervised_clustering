setup: pseudolabel
prefix: scatnetSPICE_selflabel

pretrain_path: "/home/blachm86/SPICE-main/results/stl10/scatnet_FullSelf_5head_linear/checkpoint_best.pth.tar"
pretrain_type: spice

### Technical Parameters (unimportant) FIXED
num_workers: 8

### Universal Parameters
epochs: 100
batch_size: 256

update_cluster_head_only: False
hidden_dim: 4096

#### DATASET & TRANSFORM ####

### DATASET related

## !!!fixed!!! dataset related Parameters
dataset_type: scan # Omni-compatible
num_classes: 10
train_db_name: stl-10
val_db_name: stl-10


num_neighbors: 20
to_augmented_dataset: False # mode variable
to_neighbors_dataset: False

## for stl10 dataset
# split of stl10 for training
train_split: unlabeled
# split of stl10 for validation
val_split: both

### AUGMENTATION related

# <dependent>
augmentation_type: scan 

# VERSION: default scan loss for stl_10
# Transformations
augmentation_strategy: ours 
augmentation_kwargs:
   local_crops_number: 0 
   crop_size: 96
   normalize:
      mean: [0.485, 0.456, 0.406]
      std: [0.229, 0.224, 0.225]
   num_strong_augs: 4
   cutout_kwargs:
     n_holes: 1
     length: 32
     random: True

transformation_kwargs:
   crop_size: 96
   normalize:
      mean: [0.485, 0.456, 0.406]
      std: [0.229, 0.224, 0.225]

# Hyperparameters
optimizer: adam
optimizer_kwargs:
   lr: 0.0001
   weight_decay: 0.0001
num_workers: 8

# Scheduler
scheduler: constant

### MODEL related

## backbone

# <dependent>
backbone: scatnet

feature_dim: 128
scatnet_args: 
    J: 2
    L: 16
    input_size: [96, 96, 3] 
    res_blocks: 30
    out_dim: 128

#hidden_dim: 4096 # parameter for [backbone]:twist
# <dependent>

## head

# <dependent>
num_heads: 1
model_type: spice_linearMLP
# <dependent>

model_args: # softmax is performed over loss function
    head_type: mlp
    aug_type: default
    batch_norm: False
    last_batchnorm: False
    last_activation: softmax
    drop_out: -1


### Training Algorithm with loss function

train_method: pseudolabel
loss_type: pseudolabel
entropy_weight: 5.0 # scan_loss parameter

### Training Process Parameter